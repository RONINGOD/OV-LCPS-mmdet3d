2024/04/07 13:48:44 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.8.0 (default, Nov  6 2019, 21:49:08) [GCC 7.3.0]
    CUDA available: True
    numpy_random_seed: 1872993755
    GPU 0: NVIDIA GeForce RTX 3060
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.3, V11.3.58
    GCC: gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0
    PyTorch: 1.10.1+cu111
    PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

    TorchVision: 0.11.2+cu111
    OpenCV: 4.9.0
    MMEngine: 0.7.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1872993755
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 1
------------------------------------------------------------

2024/04/07 13:48:56 - mmengine - INFO - Config:
dataset_type = '_SemanticKittiDataset'
data_root = '/home/coisini/data/kitti/dataset/'
class_names = [
    'car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person', 'bicyclist',
    'motorcyclist', 'road', 'parking', 'sidewalk', 'other-ground', 'building',
    'fence', 'vegetation', 'trunck', 'terrian', 'pole', 'traffic-sign'
]
labels_map = dict({
    0: 19,
    1: 19,
    10: 0,
    11: 1,
    13: 4,
    15: 2,
    16: 4,
    18: 3,
    20: 4,
    30: 5,
    31: 6,
    32: 7,
    40: 8,
    44: 9,
    48: 10,
    49: 11,
    50: 12,
    51: 13,
    52: 19,
    60: 8,
    70: 14,
    71: 15,
    72: 16,
    80: 17,
    81: 18,
    99: 19,
    252: 0,
    253: 6,
    254: 5,
    255: 7,
    256: 4,
    257: 4,
    258: 3,
    259: 4
})
learning_map_inv = dict({
    0: 10,
    1: 11,
    2: 15,
    3: 18,
    4: 20,
    5: 30,
    6: 31,
    7: 32,
    8: 40,
    9: 44,
    10: 48,
    11: 49,
    12: 50,
    13: 51,
    14: 70,
    15: 71,
    16: 72,
    17: 80,
    18: 81,
    19: 0
})
metainfo = dict(
    classes=[
        'car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person', 'bicyclist',
        'motorcyclist', 'road', 'parking', 'sidewalk', 'other-ground',
        'building', 'fence', 'vegetation', 'trunck', 'terrian', 'pole',
        'traffic-sign'
    ],
    seg_label_mapping=dict({
        0: 19,
        1: 19,
        10: 0,
        11: 1,
        13: 4,
        15: 2,
        16: 4,
        18: 3,
        20: 4,
        30: 5,
        31: 6,
        32: 7,
        40: 8,
        44: 9,
        48: 10,
        49: 11,
        50: 12,
        51: 13,
        52: 19,
        60: 8,
        70: 14,
        71: 15,
        72: 16,
        80: 17,
        81: 18,
        99: 19,
        252: 0,
        253: 6,
        254: 5,
        255: 7,
        256: 4,
        257: 4,
        258: 3,
        259: 4
    }),
    max_label=259)
input_modality = dict(use_lidar=True, use_camera=False)
backend_args = None
pre_transform = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=4,
        use_dim=4,
        backend_args=None),
    dict(
        type='_LoadAnnotations3D',
        with_bbox_3d=False,
        with_label_3d=False,
        with_panoptic_3d=True,
        seg_3d_dtype='np.int32',
        seg_offset=65536,
        dataset_type='semantickitti',
        backend_args=None),
    dict(type='PointSegClassMapping')
]
train_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=4,
        use_dim=4,
        backend_args=None),
    dict(
        type='_LoadAnnotations3D',
        with_bbox_3d=False,
        with_label_3d=False,
        with_panoptic_3d=True,
        seg_3d_dtype='np.int32',
        seg_offset=65536,
        dataset_type='semantickitti',
        backend_args=None),
    dict(type='PointSegClassMapping'),
    dict(
        type='RandomChoice',
        transforms=[[{
            'type':
            '_LaserMix',
            'num_areas': [3, 4, 5, 6],
            'pitch_angles': [-25, 3],
            'pre_transform': [{
                'type': 'LoadPointsFromFile',
                'coord_type': 'LIDAR',
                'load_dim': 4,
                'use_dim': 4
            }, {
                'type': '_LoadAnnotations3D',
                'with_bbox_3d': False,
                'with_label_3d': False,
                'with_panoptic_3d': True,
                'seg_3d_dtype': 'np.int32',
                'seg_offset': 65536,
                'dataset_type': 'semantickitti'
            }, {
                'type': 'PointSegClassMapping'
            }],
            'prob':
            0.5
        }],
                    [{
                        'type':
                        '_PolarMix',
                        'instance_classes': [0, 1, 2, 3, 4, 5, 6, 7],
                        'swap_ratio':
                        0.5,
                        'rotate_paste_ratio':
                        1.0,
                        'pre_transform': [{
                            'type': 'LoadPointsFromFile',
                            'coord_type': 'LIDAR',
                            'load_dim': 4,
                            'use_dim': 4
                        }, {
                            'type': '_LoadAnnotations3D',
                            'with_bbox_3d': False,
                            'with_label_3d': False,
                            'with_panoptic_3d': True,
                            'seg_3d_dtype': 'np.int32',
                            'seg_offset': 65536,
                            'dataset_type': 'semantickitti'
                        }, {
                            'type': 'PointSegClassMapping'
                        }],
                        'prob':
                        0.5
                    }]],
        prob=[0.2, 0.8]),
    dict(
        type='RandomFlip3D',
        sync_2d=False,
        flip_ratio_bev_horizontal=0.5,
        flip_ratio_bev_vertical=0.5),
    dict(
        type='GlobalRotScaleTrans',
        rot_range=[-0.78539816, 0.78539816],
        scale_ratio_range=[0.95, 1.05],
        translation_std=[0.1, 0.1, 0.1]),
    dict(
        type='Pack3DDetInputs',
        keys=['points', 'pts_semantic_mask', 'pts_instance_mask'])
]
test_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=4,
        use_dim=4,
        backend_args=None),
    dict(
        type='_LoadAnnotations3D',
        with_bbox_3d=False,
        with_label_3d=False,
        with_panoptic_3d=True,
        seg_3d_dtype='np.int32',
        seg_offset=65536,
        dataset_type='semantickitti',
        backend_args=None),
    dict(type='PointSegClassMapping'),
    dict(
        type='Pack3DDetInputs',
        keys=['points', 'pts_semantic_mask', 'pts_instance_mask'])
]
train_dataloader = dict(
    batch_size=1,
    num_workers=4,
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type='RepeatDataset',
        times=1,
        dataset=dict(
            type='_SemanticKittiDataset',
            data_root='/home/coisini/data/kitti/dataset/',
            data_prefix=dict(
                pts='',
                img='',
                pts_instance_mask='',
                pts_semantic_mask='',
                pts_panoptic_mask=''),
            ann_file='semantickitti_infos_trainval.pkl',
            pipeline=[
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=4,
                    use_dim=4,
                    backend_args=None),
                dict(
                    type='_LoadAnnotations3D',
                    with_bbox_3d=False,
                    with_label_3d=False,
                    with_panoptic_3d=True,
                    seg_3d_dtype='np.int32',
                    seg_offset=65536,
                    dataset_type='semantickitti',
                    backend_args=None),
                dict(type='PointSegClassMapping'),
                dict(
                    type='RandomChoice',
                    transforms=[[{
                        'type':
                        '_LaserMix',
                        'num_areas': [3, 4, 5, 6],
                        'pitch_angles': [-25, 3],
                        'pre_transform': [{
                            'type': 'LoadPointsFromFile',
                            'coord_type': 'LIDAR',
                            'load_dim': 4,
                            'use_dim': 4
                        }, {
                            'type': '_LoadAnnotations3D',
                            'with_bbox_3d': False,
                            'with_label_3d': False,
                            'with_panoptic_3d': True,
                            'seg_3d_dtype': 'np.int32',
                            'seg_offset': 65536,
                            'dataset_type': 'semantickitti'
                        }, {
                            'type': 'PointSegClassMapping'
                        }],
                        'prob':
                        0.5
                    }],
                                [{
                                    'type':
                                    '_PolarMix',
                                    'instance_classes':
                                    [0, 1, 2, 3, 4, 5, 6, 7],
                                    'swap_ratio':
                                    0.5,
                                    'rotate_paste_ratio':
                                    1.0,
                                    'pre_transform': [{
                                        'type': 'LoadPointsFromFile',
                                        'coord_type': 'LIDAR',
                                        'load_dim': 4,
                                        'use_dim': 4
                                    }, {
                                        'type':
                                        '_LoadAnnotations3D',
                                        'with_bbox_3d':
                                        False,
                                        'with_label_3d':
                                        False,
                                        'with_panoptic_3d':
                                        True,
                                        'seg_3d_dtype':
                                        'np.int32',
                                        'seg_offset':
                                        65536,
                                        'dataset_type':
                                        'semantickitti'
                                    }, {
                                        'type':
                                        'PointSegClassMapping'
                                    }],
                                    'prob':
                                    0.5
                                }]],
                    prob=[0.2, 0.8]),
                dict(
                    type='RandomFlip3D',
                    sync_2d=False,
                    flip_ratio_bev_horizontal=0.5,
                    flip_ratio_bev_vertical=0.5),
                dict(
                    type='GlobalRotScaleTrans',
                    rot_range=[-0.78539816, 0.78539816],
                    scale_ratio_range=[0.95, 1.05],
                    translation_std=[0.1, 0.1, 0.1]),
                dict(
                    type='Pack3DDetInputs',
                    keys=['points', 'pts_semantic_mask', 'pts_instance_mask'])
            ],
            metainfo=dict(
                classes=[
                    'car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person',
                    'bicyclist', 'motorcyclist', 'road', 'parking', 'sidewalk',
                    'other-ground', 'building', 'fence', 'vegetation',
                    'trunck', 'terrian', 'pole', 'traffic-sign'
                ],
                seg_label_mapping=dict({
                    0: 19,
                    1: 19,
                    10: 0,
                    11: 1,
                    13: 4,
                    15: 2,
                    16: 4,
                    18: 3,
                    20: 4,
                    30: 5,
                    31: 6,
                    32: 7,
                    40: 8,
                    44: 9,
                    48: 10,
                    49: 11,
                    50: 12,
                    51: 13,
                    52: 19,
                    60: 8,
                    70: 14,
                    71: 15,
                    72: 16,
                    80: 17,
                    81: 18,
                    99: 19,
                    252: 0,
                    253: 6,
                    254: 5,
                    255: 7,
                    256: 4,
                    257: 4,
                    258: 3,
                    259: 4
                }),
                max_label=259),
            modality=dict(use_lidar=True, use_camera=False),
            ignore_index=19,
            backend_args=None)))
test_dataloader = dict(
    batch_size=1,
    num_workers=1,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='RepeatDataset',
        times=1,
        dataset=dict(
            type='_SemanticKittiDataset',
            data_root='/home/coisini/data/kitti/dataset/',
            data_prefix=dict(
                pts='',
                img='',
                pts_instance_mask='',
                pts_semantic_mask='',
                pts_panoptic_mask=''),
            ann_file='semantickitti_infos_val.pkl',
            pipeline=[
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=4,
                    use_dim=4,
                    backend_args=None),
                dict(
                    type='_LoadAnnotations3D',
                    with_bbox_3d=False,
                    with_label_3d=False,
                    with_panoptic_3d=True,
                    seg_3d_dtype='np.int32',
                    seg_offset=65536,
                    dataset_type='semantickitti',
                    backend_args=None),
                dict(type='PointSegClassMapping'),
                dict(
                    type='Pack3DDetInputs',
                    keys=['points', 'pts_semantic_mask', 'pts_instance_mask'])
            ],
            metainfo=dict(
                classes=[
                    'car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person',
                    'bicyclist', 'motorcyclist', 'road', 'parking', 'sidewalk',
                    'other-ground', 'building', 'fence', 'vegetation',
                    'trunck', 'terrian', 'pole', 'traffic-sign'
                ],
                seg_label_mapping=dict({
                    0: 19,
                    1: 19,
                    10: 0,
                    11: 1,
                    13: 4,
                    15: 2,
                    16: 4,
                    18: 3,
                    20: 4,
                    30: 5,
                    31: 6,
                    32: 7,
                    40: 8,
                    44: 9,
                    48: 10,
                    49: 11,
                    50: 12,
                    51: 13,
                    52: 19,
                    60: 8,
                    70: 14,
                    71: 15,
                    72: 16,
                    80: 17,
                    81: 18,
                    99: 19,
                    252: 0,
                    253: 6,
                    254: 5,
                    255: 7,
                    256: 4,
                    257: 4,
                    258: 3,
                    259: 4
                }),
                max_label=259),
            modality=dict(use_lidar=True, use_camera=False),
            ignore_index=19,
            test_mode=True,
            backend_args=None)))
val_dataloader = dict(
    batch_size=1,
    num_workers=1,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='RepeatDataset',
        times=1,
        dataset=dict(
            type='_SemanticKittiDataset',
            data_root='/home/coisini/data/kitti/dataset/',
            data_prefix=dict(
                pts='',
                img='',
                pts_instance_mask='',
                pts_semantic_mask='',
                pts_panoptic_mask=''),
            ann_file='semantickitti_infos_val.pkl',
            pipeline=[
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=4,
                    use_dim=4,
                    backend_args=None),
                dict(
                    type='_LoadAnnotations3D',
                    with_bbox_3d=False,
                    with_label_3d=False,
                    with_panoptic_3d=True,
                    seg_3d_dtype='np.int32',
                    seg_offset=65536,
                    dataset_type='semantickitti',
                    backend_args=None),
                dict(type='PointSegClassMapping'),
                dict(
                    type='Pack3DDetInputs',
                    keys=['points', 'pts_semantic_mask', 'pts_instance_mask'])
            ],
            metainfo=dict(
                classes=[
                    'car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person',
                    'bicyclist', 'motorcyclist', 'road', 'parking', 'sidewalk',
                    'other-ground', 'building', 'fence', 'vegetation',
                    'trunck', 'terrian', 'pole', 'traffic-sign'
                ],
                seg_label_mapping=dict({
                    0: 19,
                    1: 19,
                    10: 0,
                    11: 1,
                    13: 4,
                    15: 2,
                    16: 4,
                    18: 3,
                    20: 4,
                    30: 5,
                    31: 6,
                    32: 7,
                    40: 8,
                    44: 9,
                    48: 10,
                    49: 11,
                    50: 12,
                    51: 13,
                    52: 19,
                    60: 8,
                    70: 14,
                    71: 15,
                    72: 16,
                    80: 17,
                    81: 18,
                    99: 19,
                    252: 0,
                    253: 6,
                    254: 5,
                    255: 7,
                    256: 4,
                    257: 4,
                    258: 3,
                    259: 4
                }),
                max_label=259),
            modality=dict(use_lidar=True, use_camera=False),
            ignore_index=19,
            test_mode=True,
            backend_args=None)))
val_evaluator = dict(
    type='_PanopticSegMetric',
    thing_class_inds=[0, 1, 2, 3, 4, 5, 6, 7],
    stuff_class_inds=[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],
    min_num_points=50,
    id_offset=65536,
    dataset_type='semantickitti',
    learning_map_inv=dict({
        0: 10,
        1: 11,
        2: 15,
        3: 18,
        4: 20,
        5: 30,
        6: 31,
        7: 32,
        8: 40,
        9: 44,
        10: 48,
        11: 49,
        12: 50,
        13: 51,
        14: 70,
        15: 71,
        16: 72,
        17: 80,
        18: 81,
        19: 0
    }))
test_evaluator = dict(
    type='_PanopticSegMetric',
    thing_class_inds=[0, 1, 2, 3, 4, 5, 6, 7],
    stuff_class_inds=[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],
    min_num_points=50,
    id_offset=65536,
    dataset_type='semantickitti',
    learning_map_inv=dict({
        0: 10,
        1: 11,
        2: 15,
        3: 18,
        4: 20,
        5: 30,
        6: 31,
        7: 32,
        8: 40,
        9: 44,
        10: 48,
        11: 49,
        12: 50,
        13: 51,
        14: 70,
        15: 71,
        16: 72,
        17: 80,
        18: 81,
        19: 0
    }))
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='Det3DLocalVisualizer',
    vis_backends=[dict(type='LocalVisBackend')],
    name='visualizer')
grid_shape = [480, 360, 32]
model = dict(
    type='_P3Former',
    data_preprocessor=dict(
        type='_Det3DDataPreprocessor',
        voxel=True,
        voxel_type='cylindrical',
        voxel_layer=dict(
            grid_shape=[480, 360, 32],
            point_cloud_range=[0, -3.14159265359, -4, 50, 3.14159265359, 2],
            max_num_points=-1,
            max_voxels=-1)),
    voxel_encoder=dict(
        type='SegVFE',
        feat_channels=[64, 128, 256, 256],
        in_channels=6,
        with_voxel_center=True,
        feat_compression=16,
        return_point_feats=False),
    backbone=dict(
        type='_Asymm3DSpconv',
        grid_size=[480, 360, 32],
        input_channels=16,
        base_channels=32,
        norm_cfg=dict(type='BN1d', eps=1e-05, momentum=0.1),
        more_conv=True,
        out_channels=256),
    decode_head=dict(
        type='_P3FormerHead',
        num_classes=20,
        num_queries=128,
        embed_dims=256,
        point_cloud_range=[0, -3.14159265359, -4, 50, 3.14159265359, 2],
        assigner_zero_layer_cfg=dict(
            type='mmdet.HungarianAssigner',
            match_costs=[
                dict(
                    type='mmdet.FocalLossCost',
                    weight=1.0,
                    binary_input=True,
                    gamma=2.0,
                    alpha=0.25),
                dict(type='mmdet.DiceCost', weight=2.0, pred_act=True)
            ]),
        assigner_cfg=dict(
            type='mmdet.HungarianAssigner',
            match_costs=[
                dict(
                    type='mmdet.FocalLossCost',
                    gamma=4.0,
                    alpha=0.25,
                    weight=1.0),
                dict(
                    type='mmdet.FocalLossCost',
                    weight=1.0,
                    binary_input=True,
                    gamma=2.0,
                    alpha=0.25),
                dict(type='mmdet.DiceCost', weight=2.0, pred_act=True)
            ]),
        sampler_cfg=dict(type='_MaskPseudoSampler'),
        loss_mask=dict(
            type='mmdet.FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            reduction='mean',
            loss_weight=1.0),
        loss_dice=dict(type='mmdet.DiceLoss', loss_weight=2.0),
        loss_cls=dict(
            type='mmdet.FocalLoss',
            use_sigmoid=True,
            gamma=4.0,
            alpha=0.25,
            loss_weight=1.0),
        num_decoder_layers=6,
        cls_channels=(256, 256, 20),
        mask_channels=(256, 256, 256, 256, 256),
        thing_class=[0, 1, 2, 3, 4, 5, 6, 7],
        stuff_class=[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],
        ignore_index=19),
    train_cfg=None,
    test_cfg=dict(mode='whole'))
default_scope = 'mmdet3d'
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', interval=5),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='Det3DVisualizationHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'))
log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)
log_level = 'INFO'
load_from = None
resume = False
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=36, val_interval=1)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
lr = 0.0008
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='AdamW', lr=0.0008, weight_decay=0.01))
param_scheduler = [
    dict(
        type='MultiStepLR',
        begin=0,
        end=36,
        by_epoch=True,
        milestones=[24, 32],
        gamma=0.2)
]
custom_imports = dict(
    imports=[
        'p3former.backbones.cylinder3d',
        'p3former.data_preprocessors.data_preprocessor',
        'p3former.decode_heads.p3former_head', 'p3former.segmentors.p3former',
        'p3former.task_modules.samplers.mask_pseduo_sampler',
        'evaluation.metrics.panoptic_seg_metric',
        'datasets.semantickitti_dataset', 'datasets.transforms.loading',
        'datasets.transforms.transforms_3d'
    ],
    allow_failed_imports=False)
launcher = 'pytorch'
work_dir = './work_dirs/p3former_8xb2_3x_semantickitti_trainval'

2024/04/07 13:49:12 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) Det3DVisualizationHook             
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train:
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) Det3DVisualizationHook             
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2024/04/07 13:49:25 - mmengine - WARNING - The prefix is not set in metric class _PanopticSegMetric.
Name of parameter - Initialization information

backbone.down_context.conv0_0.weight - torch.Size([1, 3, 3, 16, 32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn0_0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn0_0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.conv0_1.weight - torch.Size([3, 1, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn0_1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn0_1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.conv1_0.weight - torch.Size([3, 1, 3, 16, 32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn1_0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn1_0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.conv1_1.weight - torch.Size([1, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn1_1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_context.bn1_1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.conv0_0.weight - torch.Size([3, 1, 3, 32, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn0_0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn0_0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.conv0_1.weight - torch.Size([1, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn0_1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn0_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.conv1_0.weight - torch.Size([1, 3, 3, 32, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn1_0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn1_0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.conv1_1.weight - torch.Size([3, 1, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn1_1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.bn1_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.0.pool.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.conv0_0.weight - torch.Size([3, 1, 3, 64, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn0_0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn0_0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.conv0_1.weight - torch.Size([1, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn0_1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.conv1_0.weight - torch.Size([1, 3, 3, 64, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn1_0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn1_0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.conv1_1.weight - torch.Size([3, 1, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn1_1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.bn1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.1.pool.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.conv0_0.weight - torch.Size([3, 1, 3, 128, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn0_0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn0_0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.conv0_1.weight - torch.Size([1, 3, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn0_1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn0_1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.conv1_0.weight - torch.Size([1, 3, 3, 128, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn1_0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn1_0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.conv1_1.weight - torch.Size([3, 1, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn1_1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.bn1_1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.2.pool.weight - torch.Size([3, 3, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.conv0_0.weight - torch.Size([3, 1, 3, 256, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn0_0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn0_0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.conv0_1.weight - torch.Size([1, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn0_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn0_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.conv1_0.weight - torch.Size([1, 3, 3, 256, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn1_0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn1_0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.conv1_1.weight - torch.Size([3, 1, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn1_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.bn1_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.down_block_list.3.pool.weight - torch.Size([3, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.trans_conv.weight - torch.Size([3, 3, 3, 128, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.trans_bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.trans_bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.conv1.weight - torch.Size([1, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.conv2.weight - torch.Size([3, 1, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.conv3.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn3.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.bn3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.0.up_subm.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.trans_conv.weight - torch.Size([3, 3, 3, 256, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.trans_bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.trans_bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.conv1.weight - torch.Size([1, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.conv2.weight - torch.Size([3, 1, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.conv3.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn3.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.bn3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.1.up_subm.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.trans_conv.weight - torch.Size([3, 3, 3, 512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.trans_bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.trans_bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.conv1.weight - torch.Size([1, 3, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.conv2.weight - torch.Size([3, 1, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.conv3.weight - torch.Size([3, 3, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.2.up_subm.weight - torch.Size([3, 3, 3, 256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.trans_conv.weight - torch.Size([3, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.trans_bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.trans_bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.conv1.weight - torch.Size([1, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.conv2.weight - torch.Size([3, 1, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.conv3.weight - torch.Size([3, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.up_block_list.3.up_subm.weight - torch.Size([3, 3, 3, 512, 512]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.conv1.weight - torch.Size([3, 1, 1, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.conv2.weight - torch.Size([1, 3, 1, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.conv3.weight - torch.Size([1, 1, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn3.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.ddcm.bn3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.addConv.weight - torch.Size([3, 3, 3, 128, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.addBn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

backbone.addBn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.queries.weight - torch.Size([1, 1, 1, 256, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.sem_queries.weight - torch.Size([20, 256, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.polar_proj.weight - torch.Size([256, 3]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.polar_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.polar_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.polar_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.cart_proj.weight - torch.Size([256, 3]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.cart_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.cart_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.cart_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.pe_conv.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.pe_conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.pe_conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.0.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.1.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.2.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.3.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.4.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.dynamic_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.dynamic_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_layer.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_layer.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.update_gate.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.update_gate.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_norm_in.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_norm_in.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.input_norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.fc_layer.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.cross_attn.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.self_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.transformer_decoder.5.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.1.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.1.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.1.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.1.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.1.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.2.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.2.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.2.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.2.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.2.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.3.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.3.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.3.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.3.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.3.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.4.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.4.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.4.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.4.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.4.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.5.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.5.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.5.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.5.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.5.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.6.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.6.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.6.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.6.mlp.1.weight - torch.Size([20, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_cls.6.mlp.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.0.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.1.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.2.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.3.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.4.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.5.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_mask.6.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.0.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.1.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.2.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.3.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.4.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.5.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

decode_head.fc_coor_mask.6.mlp.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.pre_norm.weight - torch.Size([9]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.pre_norm.bias - torch.Size([9]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 9]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.0.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 64]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.1.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.2.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.vfe_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.compression_layers.0.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of _P3Former  

voxel_encoder.compression_layers.0.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of _P3Former  
2024/04/07 13:49:37 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/04/07 13:49:37 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/04/07 13:49:37 - mmengine - INFO - Checkpoints will be saved to /home/coisini/project/P3Former/work_dirs/p3former_8xb2_3x_semantickitti_trainval.
2024/04/07 14:03:34 - mmengine - INFO - Epoch(train)  [1][   50/23201]  lr: 8.0000e-04  eta: 161 days, 19:35:14  time: 16.7399  data_time: 0.0822  memory: 6747  loss: 40.3000  decode.loss_mask_0: 0.8725  decode.loss_dice_0: 1.9554  decode.loss_dice_pos_0: 0.3838  decode.loss_cls_1: 0.2583  decode.loss_mask_1: 3.3525  decode.loss_dice_1: 1.9925  decode.loss_dice_pos_1: 0.3972  decode.loss_cls_2: 0.3904  decode.loss_mask_2: 0.9451  decode.loss_dice_2: 1.9828  decode.loss_dice_pos_2: 0.3893  decode.loss_cls_3: 0.3316  decode.loss_mask_3: 2.5715  decode.loss_dice_3: 1.9857  decode.loss_dice_pos_3: 0.3974  decode.loss_cls_4: 0.3222  decode.loss_mask_4: 4.2915  decode.loss_dice_4: 1.9859  decode.loss_dice_pos_4: 0.3950  decode.loss_cls_5: 0.3915  decode.loss_mask_5: 3.1904  decode.loss_dice_5: 1.9840  decode.loss_dice_pos_5: 0.3957  decode.loss_cls_6: 0.3292  decode.loss_mask_6: 3.4488  decode.loss_dice_6: 1.9820  decode.loss_dice_pos_6: 0.3948  decode.loss_ce: 2.0709  decode.loss_lovasz: 0.9120
2024/04/07 14:04:07 - mmengine - INFO - Epoch(train)  [1][  100/23201]  lr: 8.0000e-04  eta: 84 days, 3:06:26  time: 0.6676  data_time: 0.0037  memory: 6214  loss: 24.4714  decode.loss_mask_0: 0.1422  decode.loss_dice_0: 1.8803  decode.loss_dice_pos_0: 0.3683  decode.loss_cls_1: 0.0807  decode.loss_mask_1: 0.3410  decode.loss_dice_1: 1.9891  decode.loss_dice_pos_1: 0.3774  decode.loss_cls_2: 0.0795  decode.loss_mask_2: 0.8988  decode.loss_dice_2: 1.9941  decode.loss_dice_pos_2: 0.3917  decode.loss_cls_3: 0.0798  decode.loss_mask_3: 0.6624  decode.loss_dice_3: 1.9880  decode.loss_dice_pos_3: 0.3999  decode.loss_cls_4: 0.0794  decode.loss_mask_4: 0.9463  decode.loss_dice_4: 1.9934  decode.loss_dice_pos_4: 0.3959  decode.loss_cls_5: 0.0780  decode.loss_mask_5: 0.8583  decode.loss_dice_5: 1.9972  decode.loss_dice_pos_5: 0.4000  decode.loss_cls_6: 0.0801  decode.loss_mask_6: 0.9430  decode.loss_dice_6: 1.9936  decode.loss_dice_pos_6: 0.3984  decode.loss_ce: 1.7388  decode.loss_lovasz: 0.8959
2024/04/07 14:04:40 - mmengine - INFO - Epoch(train)  [1][  150/23201]  lr: 8.0000e-04  eta: 58 days, 4:53:13  time: 0.6582  data_time: 0.0037  memory: 6314  loss: 22.1031  decode.loss_mask_0: 0.1437  decode.loss_dice_0: 1.8288  decode.loss_dice_pos_0: 0.3648  decode.loss_cls_1: 0.0741  decode.loss_mask_1: 0.3328  decode.loss_dice_1: 1.9916  decode.loss_dice_pos_1: 0.3685  decode.loss_cls_2: 0.0790  decode.loss_mask_2: 0.3187  decode.loss_dice_2: 1.9867  decode.loss_dice_pos_2: 0.3677  decode.loss_cls_3: 0.0788  decode.loss_mask_3: 0.5219  decode.loss_dice_3: 1.9946  decode.loss_dice_pos_3: 0.3936  decode.loss_cls_4: 0.0784  decode.loss_mask_4: 0.6137  decode.loss_dice_4: 1.9924  decode.loss_dice_pos_4: 0.3855  decode.loss_cls_5: 0.0781  decode.loss_mask_5: 0.6627  decode.loss_dice_5: 1.9853  decode.loss_dice_pos_5: 0.3918  decode.loss_cls_6: 0.0802  decode.loss_mask_6: 0.2024  decode.loss_dice_6: 1.9622  decode.loss_dice_pos_6: 0.3673  decode.loss_ce: 1.5846  decode.loss_lovasz: 0.8732
2024/04/07 14:05:14 - mmengine - INFO - Epoch(train)  [1][  200/23201]  lr: 8.0000e-04  eta: 45 days, 7:01:22  time: 0.6798  data_time: 0.0037  memory: 6982  loss: 20.5087  decode.loss_mask_0: 0.0878  decode.loss_dice_0: 1.6872  decode.loss_dice_pos_0: 0.3501  decode.loss_cls_1: 0.0745  decode.loss_mask_1: 0.1377  decode.loss_dice_1: 1.9492  decode.loss_dice_pos_1: 0.3679  decode.loss_cls_2: 0.0777  decode.loss_mask_2: 0.5378  decode.loss_dice_2: 1.9941  decode.loss_dice_pos_2: 0.3661  decode.loss_cls_3: 0.0784  decode.loss_mask_3: 0.3870  decode.loss_dice_3: 1.9725  decode.loss_dice_pos_3: 0.3847  decode.loss_cls_4: 0.0792  decode.loss_mask_4: 0.2182  decode.loss_dice_4: 1.9713  decode.loss_dice_pos_4: 0.3677  decode.loss_cls_5: 0.0773  decode.loss_mask_5: 0.1638  decode.loss_dice_5: 1.9175  decode.loss_dice_pos_5: 0.3726  decode.loss_cls_6: 0.0795  decode.loss_mask_6: 0.1839  decode.loss_dice_6: 1.9706  decode.loss_dice_pos_6: 0.3650  decode.loss_ce: 1.4426  decode.loss_lovasz: 0.8468
2024/04/07 14:05:47 - mmengine - INFO - Epoch(train)  [1][  250/23201]  lr: 8.0000e-04  eta: 37 days, 12:27:10  time: 0.6658  data_time: 0.0036  memory: 6537  loss: 18.9612  decode.loss_mask_0: 0.0577  decode.loss_dice_0: 1.5814  decode.loss_dice_pos_0: 0.3391  decode.loss_cls_1: 0.0723  decode.loss_mask_1: 0.0580  decode.loss_dice_1: 1.8368  decode.loss_dice_pos_1: 0.3693  decode.loss_cls_2: 0.0777  decode.loss_mask_2: 0.1020  decode.loss_dice_2: 1.9416  decode.loss_dice_pos_2: 0.3693  decode.loss_cls_3: 0.0795  decode.loss_mask_3: 0.1506  decode.loss_dice_3: 1.9318  decode.loss_dice_pos_3: 0.3735  decode.loss_cls_4: 0.0798  decode.loss_mask_4: 0.1167  decode.loss_dice_4: 1.9650  decode.loss_dice_pos_4: 0.3693  decode.loss_cls_5: 0.0789  decode.loss_mask_5: 0.1311  decode.loss_dice_5: 1.8911  decode.loss_dice_pos_5: 0.3699  decode.loss_cls_6: 0.0794  decode.loss_mask_6: 0.1013  decode.loss_dice_6: 1.9318  decode.loss_dice_pos_6: 0.3692  decode.loss_ce: 1.3046  decode.loss_lovasz: 0.8326
2024/04/07 14:06:21 - mmengine - INFO - Epoch(train)  [1][  300/23201]  lr: 8.0000e-04  eta: 32 days, 8:19:27  time: 0.6724  data_time: 0.0036  memory: 6774  loss: 18.5476  decode.loss_mask_0: 0.0482  decode.loss_dice_0: 1.5254  decode.loss_dice_pos_0: 0.3372  decode.loss_cls_1: 0.0741  decode.loss_mask_1: 0.0525  decode.loss_dice_1: 1.7746  decode.loss_dice_pos_1: 0.3692  decode.loss_cls_2: 0.0766  decode.loss_mask_2: 0.0702  decode.loss_dice_2: 1.8626  decode.loss_dice_pos_2: 0.3692  decode.loss_cls_3: 0.0790  decode.loss_mask_3: 0.0729  decode.loss_dice_3: 1.8723  decode.loss_dice_pos_3: 0.3577  decode.loss_cls_4: 0.0787  decode.loss_mask_4: 0.0988  decode.loss_dice_4: 1.9332  decode.loss_dice_pos_4: 0.3692  decode.loss_cls_5: 0.0786  decode.loss_mask_5: 0.1827  decode.loss_dice_5: 1.9512  decode.loss_dice_pos_5: 0.3738  decode.loss_cls_6: 0.0787  decode.loss_mask_6: 0.1086  decode.loss_dice_6: 1.8953  decode.loss_dice_pos_6: 0.3686  decode.loss_ce: 1.2638  decode.loss_lovasz: 0.8245
2024/04/07 14:06:54 - mmengine - INFO - Epoch(train)  [1][  350/23201]  lr: 8.0000e-04  eta: 28 days, 15:32:39  time: 0.6690  data_time: 0.0037  memory: 7113  loss: 17.7196  decode.loss_mask_0: 0.0424  decode.loss_dice_0: 1.4359  decode.loss_dice_pos_0: 0.3319  decode.loss_cls_1: 0.0765  decode.loss_mask_1: 0.0326  decode.loss_dice_1: 1.6672  decode.loss_dice_pos_1: 0.3681  decode.loss_cls_2: 0.0775  decode.loss_mask_2: 0.0592  decode.loss_dice_2: 1.8391  decode.loss_dice_pos_2: 0.3681  decode.loss_cls_3: 0.0800  decode.loss_mask_3: 0.0462  decode.loss_dice_3: 1.8145  decode.loss_dice_pos_3: 0.3546  decode.loss_cls_4: 0.0803  decode.loss_mask_4: 0.0716  decode.loss_dice_4: 1.8656  decode.loss_dice_pos_4: 0.3681  decode.loss_cls_5: 0.0797  decode.loss_mask_5: 0.0544  decode.loss_dice_5: 1.8254  decode.loss_dice_pos_5: 0.3552  decode.loss_cls_6: 0.0800  decode.loss_mask_6: 0.0769  decode.loss_dice_6: 1.8737  decode.loss_dice_pos_6: 0.3679  decode.loss_ce: 1.2285  decode.loss_lovasz: 0.7986
2024/04/07 14:07:28 - mmengine - INFO - Epoch(train)  [1][  400/23201]  lr: 8.0000e-04  eta: 25 days, 21:22:54  time: 0.6836  data_time: 0.0036  memory: 6342  loss: 17.8565  decode.loss_mask_0: 0.0433  decode.loss_dice_0: 1.3924  decode.loss_dice_pos_0: 0.3315  decode.loss_cls_1: 0.0843  decode.loss_mask_1: 0.0305  decode.loss_dice_1: 1.5726  decode.loss_dice_pos_1: 0.3674  decode.loss_cls_2: 0.0896  decode.loss_mask_2: 0.0860  decode.loss_dice_2: 1.8761  decode.loss_dice_pos_2: 0.3674  decode.loss_cls_3: 0.0868  decode.loss_mask_3: 0.0515  decode.loss_dice_3: 1.8185  decode.loss_dice_pos_3: 0.3550  decode.loss_cls_4: 0.0906  decode.loss_mask_4: 0.0748  decode.loss_dice_4: 1.8839  decode.loss_dice_pos_4: 0.3674  decode.loss_cls_5: 0.0911  decode.loss_mask_5: 0.0882  decode.loss_dice_5: 1.8835  decode.loss_dice_pos_5: 0.3625  decode.loss_cls_6: 0.0895  decode.loss_mask_6: 0.1598  decode.loss_dice_6: 1.9241  decode.loss_dice_pos_6: 0.3638  decode.loss_ce: 1.1288  decode.loss_lovasz: 0.7958
2024/04/07 14:08:02 - mmengine - INFO - Epoch(train)  [1][  450/23201]  lr: 8.0000e-04  eta: 23 days, 17:23:59  time: 0.6634  data_time: 0.0036  memory: 6904  loss: 17.9186  decode.loss_mask_0: 0.0471  decode.loss_dice_0: 1.3299  decode.loss_dice_pos_0: 0.3214  decode.loss_cls_1: 0.0832  decode.loss_mask_1: 0.0728  decode.loss_dice_1: 1.6092  decode.loss_dice_pos_1: 0.3622  decode.loss_cls_2: 0.0880  decode.loss_mask_2: 0.1168  decode.loss_dice_2: 1.8584  decode.loss_dice_pos_2: 0.3622  decode.loss_cls_3: 0.0869  decode.loss_mask_3: 0.0873  decode.loss_dice_3: 1.8236  decode.loss_dice_pos_3: 0.3504  decode.loss_cls_4: 0.0871  decode.loss_mask_4: 0.0821  decode.loss_dice_4: 1.8362  decode.loss_dice_pos_4: 0.3622  decode.loss_cls_5: 0.0870  decode.loss_mask_5: 0.1100  decode.loss_dice_5: 1.8699  decode.loss_dice_pos_5: 0.3561  decode.loss_cls_6: 0.0876  decode.loss_mask_6: 0.1947  decode.loss_dice_6: 1.9561  decode.loss_dice_pos_6: 0.3570  decode.loss_ce: 1.1595  decode.loss_lovasz: 0.7735
2024/04/07 14:08:35 - mmengine - INFO - Epoch(train)  [1][  500/23201]  lr: 8.0000e-04  eta: 21 days, 23:54:29  time: 0.6676  data_time: 0.0036  memory: 6892  loss: 16.9966  decode.loss_mask_0: 0.0319  decode.loss_dice_0: 1.3265  decode.loss_dice_pos_0: 0.3332  decode.loss_cls_1: 0.0718  decode.loss_mask_1: 0.0299  decode.loss_dice_1: 1.5755  decode.loss_dice_pos_1: 0.3718  decode.loss_cls_2: 0.0723  decode.loss_mask_2: 0.0352  decode.loss_dice_2: 1.6884  decode.loss_dice_pos_2: 0.3718  decode.loss_cls_3: 0.0750  decode.loss_mask_3: 0.0401  decode.loss_dice_3: 1.8021  decode.loss_dice_pos_3: 0.3585  decode.loss_cls_4: 0.0752  decode.loss_mask_4: 0.0377  decode.loss_dice_4: 1.8021  decode.loss_dice_pos_4: 0.3718  decode.loss_cls_5: 0.0752  decode.loss_mask_5: 0.0450  decode.loss_dice_5: 1.8174  decode.loss_dice_pos_5: 0.3654  decode.loss_cls_6: 0.0752  decode.loss_mask_6: 0.0682  decode.loss_dice_6: 1.8712  decode.loss_dice_pos_6: 0.3707  decode.loss_ce: 1.0692  decode.loss_lovasz: 0.7681
2024/04/07 14:09:09 - mmengine - INFO - Epoch(train)  [1][  550/23201]  lr: 8.0000e-04  eta: 20 days, 14:25:38  time: 0.6898  data_time: 0.0036  memory: 6559  loss: 16.7794  decode.loss_mask_0: 0.0347  decode.loss_dice_0: 1.3421  decode.loss_dice_pos_0: 0.3304  decode.loss_cls_1: 0.0567  decode.loss_mask_1: 0.0289  decode.loss_dice_1: 1.5264  decode.loss_dice_pos_1: 0.3696  decode.loss_cls_2: 0.0651  decode.loss_mask_2: 0.0375  decode.loss_dice_2: 1.5989  decode.loss_dice_pos_2: 0.3697  decode.loss_cls_3: 0.0734  decode.loss_mask_3: 0.0311  decode.loss_dice_3: 1.7230  decode.loss_dice_pos_3: 0.3527  decode.loss_cls_4: 0.0746  decode.loss_mask_4: 0.0763  decode.loss_dice_4: 1.8536  decode.loss_dice_pos_4: 0.3697  decode.loss_cls_5: 0.0744  decode.loss_mask_5: 0.0387  decode.loss_dice_5: 1.8042  decode.loss_dice_pos_5: 0.3559  decode.loss_cls_6: 0.0746  decode.loss_mask_6: 0.0593  decode.loss_dice_6: 1.8215  decode.loss_dice_pos_6: 0.3692  decode.loss_ce: 1.0979  decode.loss_lovasz: 0.7693
2024/04/07 14:09:43 - mmengine - INFO - Epoch(train)  [1][  600/23201]  lr: 8.0000e-04  eta: 19 days, 9:58:41  time: 0.6615  data_time: 0.0036  memory: 5917  loss: 16.3909  decode.loss_mask_0: 0.0317  decode.loss_dice_0: 1.2869  decode.loss_dice_pos_0: 0.3277  decode.loss_cls_1: 0.0598  decode.loss_mask_1: 0.0341  decode.loss_dice_1: 1.5162  decode.loss_dice_pos_1: 0.3709  decode.loss_cls_2: 0.0609  decode.loss_mask_2: 0.0319  decode.loss_dice_2: 1.5540  decode.loss_dice_pos_2: 0.3710  decode.loss_cls_3: 0.0652  decode.loss_mask_3: 0.0347  decode.loss_dice_3: 1.6391  decode.loss_dice_pos_3: 0.3520  decode.loss_cls_4: 0.0763  decode.loss_mask_4: 0.0369  decode.loss_dice_4: 1.8001  decode.loss_dice_pos_4: 0.3710  decode.loss_cls_5: 0.0764  decode.loss_mask_5: 0.0415  decode.loss_dice_5: 1.8104  decode.loss_dice_pos_5: 0.3572  decode.loss_cls_6: 0.0768  decode.loss_mask_6: 0.0416  decode.loss_dice_6: 1.8042  decode.loss_dice_pos_6: 0.3701  decode.loss_ce: 1.0354  decode.loss_lovasz: 0.7571
2024/04/07 14:10:17 - mmengine - INFO - Epoch(train)  [1][  650/23201]  lr: 8.0000e-04  eta: 18 days, 10:24:40  time: 0.6899  data_time: 0.0036  memory: 6831  loss: 16.1143  decode.loss_mask_0: 0.0357  decode.loss_dice_0: 1.2772  decode.loss_dice_pos_0: 0.3256  decode.loss_cls_1: 0.0594  decode.loss_mask_1: 0.0269  decode.loss_dice_1: 1.4682  decode.loss_dice_pos_1: 0.3700  decode.loss_cls_2: 0.0566  decode.loss_mask_2: 0.0255  decode.loss_dice_2: 1.4796  decode.loss_dice_pos_2: 0.3702  decode.loss_cls_3: 0.0596  decode.loss_mask_3: 0.0341  decode.loss_dice_3: 1.5402  decode.loss_dice_pos_3: 0.3486  decode.loss_cls_4: 0.0691  decode.loss_mask_4: 0.0576  decode.loss_dice_4: 1.7778  decode.loss_dice_pos_4: 0.3702  decode.loss_cls_5: 0.0785  decode.loss_mask_5: 0.0374  decode.loss_dice_5: 1.7897  decode.loss_dice_pos_5: 0.3560  decode.loss_cls_6: 0.0788  decode.loss_mask_6: 0.0496  decode.loss_dice_6: 1.8133  decode.loss_dice_pos_6: 0.3661  decode.loss_ce: 1.0379  decode.loss_lovasz: 0.7550
2024/04/07 14:10:50 - mmengine - INFO - Epoch(train)  [1][  700/23201]  lr: 8.0000e-04  eta: 17 days, 13:49:39  time: 0.6668  data_time: 0.0036  memory: 6459  loss: 15.7810  decode.loss_mask_0: 0.0347  decode.loss_dice_0: 1.2478  decode.loss_dice_pos_0: 0.3259  decode.loss_cls_1: 0.0582  decode.loss_mask_1: 0.0276  decode.loss_dice_1: 1.4329  decode.loss_dice_pos_1: 0.3674  decode.loss_cls_2: 0.0556  decode.loss_mask_2: 0.0333  decode.loss_dice_2: 1.4355  decode.loss_dice_pos_2: 0.3678  decode.loss_cls_3: 0.0596  decode.loss_mask_3: 0.0277  decode.loss_dice_3: 1.4721  decode.loss_dice_pos_3: 0.3442  decode.loss_cls_4: 0.0778  decode.loss_mask_4: 0.0490  decode.loss_dice_4: 1.7231  decode.loss_dice_pos_4: 0.3678  decode.loss_cls_5: 0.0770  decode.loss_mask_5: 0.0453  decode.loss_dice_5: 1.7933  decode.loss_dice_pos_5: 0.3541  decode.loss_cls_6: 0.0777  decode.loss_mask_6: 0.0427  decode.loss_dice_6: 1.7900  decode.loss_dice_pos_6: 0.3556  decode.loss_ce: 0.9971  decode.loss_lovasz: 0.7403
2024/04/07 14:11:24 - mmengine - INFO - Epoch(train)  [1][  750/23201]  lr: 8.0000e-04  eta: 16 days, 19:59:42  time: 0.6674  data_time: 0.0036  memory: 6821  loss: 15.4365  decode.loss_mask_0: 0.0331  decode.loss_dice_0: 1.2127  decode.loss_dice_pos_0: 0.3224  decode.loss_cls_1: 0.0511  decode.loss_mask_1: 0.0376  decode.loss_dice_1: 1.4116  decode.loss_dice_pos_1: 0.3674  decode.loss_cls_2: 0.0484  decode.loss_mask_2: 0.0371  decode.loss_dice_2: 1.4318  decode.loss_dice_pos_2: 0.3687  decode.loss_cls_3: 0.0488  decode.loss_mask_3: 0.0383  decode.loss_dice_3: 1.4488  decode.loss_dice_pos_3: 0.3417  decode.loss_cls_4: 0.0595  decode.loss_mask_4: 0.0438  decode.loss_dice_4: 1.5850  decode.loss_dice_pos_4: 0.3687  decode.loss_cls_5: 0.0647  decode.loss_mask_5: 0.0281  decode.loss_dice_5: 1.6839  decode.loss_dice_pos_5: 0.3529  decode.loss_cls_6: 0.0732  decode.loss_mask_6: 0.0408  decode.loss_dice_6: 1.7923  decode.loss_dice_pos_6: 0.3563  decode.loss_ce: 1.0382  decode.loss_lovasz: 0.7496
2024/04/07 14:11:59 - mmengine - INFO - Epoch(train)  [1][  800/23201]  lr: 8.0000e-04  eta: 16 days, 4:48:42  time: 0.6964  data_time: 0.0036  memory: 7185  loss: 15.1685  decode.loss_mask_0: 0.0295  decode.loss_dice_0: 1.2430  decode.loss_dice_pos_0: 0.3290  decode.loss_cls_1: 0.0553  decode.loss_mask_1: 0.0248  decode.loss_dice_1: 1.4000  decode.loss_dice_pos_1: 0.3652  decode.loss_cls_2: 0.0521  decode.loss_mask_2: 0.0331  decode.loss_dice_2: 1.4284  decode.loss_dice_pos_2: 0.3711  decode.loss_cls_3: 0.0500  decode.loss_mask_3: 0.0303  decode.loss_dice_3: 1.4498  decode.loss_dice_pos_3: 0.3440  decode.loss_cls_4: 0.0512  decode.loss_mask_4: 0.0270  decode.loss_dice_4: 1.4806  decode.loss_dice_pos_4: 0.3711  decode.loss_cls_5: 0.0585  decode.loss_mask_5: 0.0279  decode.loss_dice_5: 1.6199  decode.loss_dice_pos_5: 0.3527  decode.loss_cls_6: 0.0783  decode.loss_mask_6: 0.0309  decode.loss_dice_6: 1.7901  decode.loss_dice_pos_6: 0.3587  decode.loss_ce: 0.9777  decode.loss_lovasz: 0.7383
2024/04/07 14:12:31 - mmengine - INFO - Epoch(train)  [1][  850/23201]  lr: 8.0000e-04  eta: 15 days, 14:42:30  time: 0.6447  data_time: 0.0036  memory: 6516  loss: 14.2801  decode.loss_mask_0: 0.0262  decode.loss_dice_0: 1.1166  decode.loss_dice_pos_0: 0.3196  decode.loss_cls_1: 0.0533  decode.loss_mask_1: 0.0333  decode.loss_dice_1: 1.3043  decode.loss_dice_pos_1: 0.3429  decode.loss_cls_2: 0.0505  decode.loss_mask_2: 0.0399  decode.loss_dice_2: 1.3476  decode.loss_dice_pos_2: 0.3671  decode.loss_cls_3: 0.0490  decode.loss_mask_3: 0.0324  decode.loss_dice_3: 1.3563  decode.loss_dice_pos_3: 0.3340  decode.loss_cls_4: 0.0478  decode.loss_mask_4: 0.0345  decode.loss_dice_4: 1.3837  decode.loss_dice_pos_4: 0.3670  decode.loss_cls_5: 0.0502  decode.loss_mask_5: 0.0390  decode.loss_dice_5: 1.4864  decode.loss_dice_pos_5: 0.3442  decode.loss_cls_6: 0.0591  decode.loss_mask_6: 0.0339  decode.loss_dice_6: 1.6702  decode.loss_dice_pos_6: 0.3498  decode.loss_ce: 0.9352  decode.loss_lovasz: 0.7062
